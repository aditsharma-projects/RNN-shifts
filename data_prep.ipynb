{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Generate deterministic file name from configuration\n",
        "def generate_file_names(preprocessed_dir, nrows, fill_missing_shifts, normalize):\n",
        "    name = preprocessed_dir + 'pbj'\n",
        "    if nrows is not None:\n",
        "        name += f\"_nrows_{nrows}\"\n",
        "    if fill_missing_shifts:\n",
        "        name += \"_zeros\"\n",
        "    if normalize:\n",
        "        name += \"_norm\"\n",
        "    return name + '.csv', name + '.pickle'\n",
        "\n",
        "# Print s if conditional is truthy\n",
        "def print_if(s, conditional):\n",
        "    if conditional:\n",
        "        print(s)\n",
        "\n",
        "# Pad insert rows between each employee's start and end days with 0 hours\n",
        "def do_fill_missing_shifts(df, verbose):\n",
        "    print_if(\"Filling missing shifts...\", verbose)\n",
        "\n",
        "    # Partition by employee id\n",
        "    grouped = df.groupby('employee_id')\n",
        "\n",
        "    partitions = []\n",
        "    for name, group in grouped:\n",
        "        # Sort by date\n",
        "        group = group.sort_values(by='date')\n",
        "        day = min(group['date'])\n",
        "        for i in range(len(group)):\n",
        "            row_copy = {key:value for key, value in group.iloc[0].items()}\n",
        "            row_copy['hours'] = 0\n",
        "            # Catch date up to current index by filling in with cached rows\n",
        "            while day < group.iloc[i]['date']:\n",
        "                row_copy['date'] = day\n",
        "                group = group.append({key:value for key, value in row_copy.items()}, ignore_index=True)\n",
        "                day += pd.DateOffset(1)\n",
        "\n",
        "            # Account for current index's day\n",
        "            day += pd.DateOffset(1)\n",
        "\n",
        "        # Sort by date with new rows\n",
        "        group = group.sort_values(by='date')\n",
        "        partitions.append(group)\n",
        "\n",
        "    return pd.concat(partitions)\n",
        "\n",
        "def initial_preprocess(raw_path, preprocessed_dir, nrows=None, fill_missing_shifts=False, verbose=True, normalize=False):\n",
        "    data_file, info_file = generate_file_names(preprocessed_dir, nrows, fill_missing_shifts, normalize)\n",
        "\n",
        "    try:\n",
        "        print_if(f\"Loading preprocessed data from '{data_file}'...\", verbose)\n",
        "        df = pd.read_csv(data_file)\n",
        "\n",
        "        print_if(f\"Loading related info from '{info_file}'...\", verbose)\n",
        "        with open(info_file, 'rb') as f:\n",
        "            info = pickle.load(f)\n",
        "        return df, info\n",
        "    except FileNotFoundError:\n",
        "        print_if(\"Failed.\", verbose)\n",
        "    \n",
        "    print_if(\"Loading data...\", verbose)\n",
        "    df = pd.read_csv(raw_path, nrows=nrows, dtype={'hours':'float64'}, parse_dates = ['date'])\n",
        "    info = {}\n",
        "    if fill_missing_shifts:\n",
        "        df = do_fill_missing_shifts(df, verbose)\n",
        "    \n",
        "    if normalize:\n",
        "        # Normalize features\n",
        "        means = {}\n",
        "        stds = {}\n",
        "        #df = df.filter(['hours', 'date_int'])\n",
        "        for col in ['hours']:\n",
        "            means[col] = df[col].mean()\n",
        "            stds[col] = df[col].std()\n",
        "            df[col] = (df[col] - means[col]) / stds[col]\n",
        "        info['means'] = means\n",
        "        info['stds'] = stds\n",
        "\n",
        "    print_if(\"Saving preprocessed data...\", verbose)\n",
        "    df.to_csv(data_file, index=False)\n",
        "\n",
        "    with open(info_file, 'wb') as handle:\n",
        "        pickle.dump(info, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    \n",
        "    print_if(\"Preprocessing finished.\", verbose)\n",
        "    return df, info"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}