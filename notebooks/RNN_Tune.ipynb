{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "### Testing effect of recurrance length on prediction performance\n",
    "\n",
    "## Inputs\n",
    "The input variables for the RNN's in this notebook are the following:\n",
    "\n",
    "    1. Time series data -- Variable\n",
    "    2. Additional predictors -- day_of_week, avg_employees, perc_hours_today_before,\n",
    "       perc_hours_yesterday_before, perc_hours_tomorrow_before\n",
    "\n",
    "## Model Architecture\n",
    "The architecture used in this notebook combines an RNN with a feed forward neural network. The RNN layer recieves n days of lagged shift data and makes a prediction for the shift on the n+1th day. This prediction is then concatenated with the additional predictors and fed into a traditional neural network(various shapes, see log files for all tested archectures) to generate a better prediction--the idea being that the RNN (through it's long and short term memory) learns patterns over time and the FF network adjusts these patterns based on additional information. \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"/RNN-shifts/notebooks/Images/Unconditioned_RNN_Diagram.jpeg\"\n" width="800" />
   
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logs hyperparameter specifications and other attributes of each run into a csv file\n",
    "def log_model_info(model_info, path):\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except FileNotFoundError:\n",
    "        #print(f\"History csv not found at {path}. Creating new file.\")\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    new_df = pd.DataFrame({k: [v] for k, v in model_info.items()})\n",
    "    df = pd.concat([df, new_df], axis=0)\n",
    "    df.to_csv(path, index=False)\n",
    "    \n",
    "#Expand categorical variables enumerated in labels to one-hot encodings\n",
    "#Takes in pandas dataframe and returns tf tensor \n",
    "#Column ordering is preservered, with the convereted categorical columns dropped from the frame\n",
    "#and their one-hot encodings concatenated to the end of the converted tensor\n",
    "def expand_one_hot(labels,dataset):\n",
    "    outList = []\n",
    "    for label in labels:  \n",
    "        col = dataset[label]\n",
    "        ###Generate a dict for all unique values (Don't waste space encoding non important job id's)\n",
    "        map = {}\n",
    "        index = 0\n",
    "        for element in col.unique():\n",
    "            map[element] = index\n",
    "            index += 1\n",
    "        col = col.map(map)\n",
    "        tensor = tf.one_hot(col,len(col.unique()))\n",
    "        outList.append(tensor)\n",
    "        dataset = dataset.drop(columns=[label])\n",
    "    \n",
    "    outList.insert(0,dataset)\n",
    "    output = tf.concat(outList,1)\n",
    "    return output\n",
    "\n",
    "#Loads and preprocesses data from training_set.csv and crossvalidation_set.csv\n",
    "def get_data():\n",
    "    #nRows = 10000\n",
    "    include_fields = ['hours','day_of_week','avg_employees','perc_hours_today_before',\n",
    "                      'perc_hours_yesterday_before', 'perc_hours_tomorrow_before']\n",
    "    for i in range(1,15):\n",
    "        include_fields.append(f\"hours_l{i}\")\n",
    "    \n",
    "    train = pd.read_csv(\"/export/storage_adgandhi/PBJhours_ML/Data/Intermediate/train_test_validation/training_set.csv\",usecols=include_fields).dropna()\n",
    "    val = pd.read_csv(\"/export/storage_adgandhi/PBJhours_ML/Data/Intermediate/train_test_validation/crossvalidation_set.csv\",usecols=include_fields).dropna()\n",
    "    \n",
    "    train_inputs, train_labels = train.drop(['hours'], axis=1), train.filter(['hours'])\n",
    "    val_inputs, val_labels = val.drop(['hours'], axis=1), val.filter(['hours'])\n",
    "    train_inputs = expand_one_hot(['day_of_week'],train_inputs)\n",
    "    val_inputs = expand_one_hot(['day_of_week'],val_inputs)\n",
    "    \n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 256\n",
    "    trainSet = tf.data.Dataset.from_tensor_slices((train_inputs,train_labels)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    valSet = tf.data.Dataset.from_tensor_slices((val_inputs,val_labels)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    return trainSet,valSet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN class, defines attributes of a model\n",
    "class RNN(tf.keras.Model):\n",
    "    #define all components of model\n",
    "    def __init__(self,recurrance_length):\n",
    "        super(RNN, self).__init__()\n",
    "        self.recurrance_length = recurrance_length\n",
    "        self.lstm = tf.keras.layers.LSTM(32)\n",
    "        self.dense1 = tf.keras.layers.Dense(16, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(8, activation=tf.nn.relu)\n",
    "        self.out = tf.keras.layers.Dense(1)\n",
    "        self.batch_norm1 = tf.keras.layers.BatchNormalization()\n",
    "        self.batch_norm2 = tf.keras.layers.BatchNormalization()\n",
    "    \n",
    "    #define model architecture\n",
    "    def call(self, inputs, training=False):\n",
    "        #prov_id is first column\n",
    "        time_series = tf.reverse(tf.expand_dims(inputs[:,0:int(self.recurrance_length)],2),[1])\n",
    "        additional_inputs = inputs[:,14:]\n",
    "\n",
    "        x = self.lstm(time_series)\n",
    "        x = tf.concat([x,additional_inputs],1)\n",
    "        x = self.dense1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "# Callback function to decrease learning rate\n",
    "def decay(epoch):\n",
    "  if epoch < 3:\n",
    "    return learning_rate\n",
    "  elif epoch >= 3 and epoch < 7:\n",
    "    return learning_rate/10\n",
    "  else:\n",
    "    return learning_rate/100\n",
    "\n",
    "# base dict of values to log for each run. Some values are common to every run\n",
    "base_dict = {'Recurrance length':-1,'LSTM Units':32,'Embedding Dimension':0,'FF model shape':[16,8,1],'Initial Learning Rate':learning_rate,'Regularization':\"Batch Normalization\",'Metric':\"mse\",'Training loss':-1,'Val loss':-1,\n",
    "             'time_start':-1,'time_duration':-1,'Epochs':10,'Columns':['day_of_week','avg_employees','perc_hours_today_before',\n",
    "             'perc_hours_yesterday_before', 'perc_hours_tomorrow_before'],'LSTM type':\"Unconditioned\",'user':\"asharma\"}\n",
    "\n",
    "# Worker function for multiprocessing Process. Trains an RNN with the specified recurrance length\n",
    "def train_and_test_models(recurrance_length,garb):\n",
    "    print(f\"Started process with recurrance length: {recurrance_length}\")\n",
    "    trainSet,valSet = get_data()\n",
    "    start_time = time.time()\n",
    "    start_date = datetime.datetime.now()\n",
    "    model = RNN(recurrance_length)\n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "    callbacks = [\n",
    "    tf.keras.callbacks.LearningRateScheduler(decay)\n",
    "    ]\n",
    "    #print(\"CHECKPOINT 2\")\n",
    "    #print(trainSet)\n",
    "    history = model.fit(trainSet, epochs=10, callbacks=callbacks, validation_data=valSet,verbose=0)\n",
    "    #print(\"CHECKPOINT 3\")\n",
    "    valLoss, metric = model.evaluate(valSet)\n",
    "    #print(\"CHECKPOINT 4\")\n",
    "    param_dict = base_dict.copy()\n",
    "    time_taken = str(datetime.timedelta(seconds=(time.time()-start_time)))\n",
    "    train_loss = history.history['loss'][9]\n",
    "    param_dict['Recurrance length'] = recurrance_length\n",
    "    param_dict['Training loss'] = train_loss\n",
    "    param_dict['Val loss'] = valLoss\n",
    "    param_dict['time_start'] = start_date\n",
    "    param_dict['time_duration'] = time_taken\n",
    "    log_model_info(param_dict,'/users/facsupport/asharma/RNN-shifts/rnn_tuning_history.csv')\n",
    "    print(\"COMPLETED WORK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started process with recurrance length: 3\n",
      "Started process with recurrance length: 7\n",
      "Started process with recurrance length: 10\n",
      "Started process with recurrance length: 14\n"
     ]
    }
   ],
   "source": [
    "lengths = [3,7,10,14]\n",
    "tasks = []\n",
    "for i in range(len(lengths)):\n",
    "    tasks.append(Process(target=train_and_test_models,args=(lengths[i],0)))\n",
    "    tasks[i].start()\n",
    "for i in range(len(lengths)):\n",
    "    tasks[i].join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/users/facsupport/asharma/RNN-shifts/rnn_tuning_history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6",
   "language": "python",
   "name": "testproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
