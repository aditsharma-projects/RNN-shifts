{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "def normalize(dataset):\n",
    "    seriesIndices = ['hours']\n",
    "    for i in range(1,31):\n",
    "        seriesIndices.append(f\"hours_l{i}\")\n",
    "    dataset[seriesIndices] = dataset[seriesIndices].apply(lambda x: (x - x.mean()) / (x.std()))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "ROWS = 10 ** 8\n",
    "train = pd.read_csv(\"/export/storage_adgandhi/PBJhours_ML/Data/Intermediate/train_test_validation/training_set.csv\",nrows=ROWS)\n",
    "dropList = []\n",
    "for i in range(1,31):\n",
    "    dropList.append(f\"employees_l{i}\")\n",
    "#train = train.drop(dropList,axis=1)\n",
    "train = train.dropna()\n",
    "#train = normalize(train)\n",
    "\n",
    "#val = pd.read_csv(\"/export/storage_adgandhi/PBJhours_ML/Data/Intermediate/train_test_validation/validation_set.csv\",nrows=ROWS/2,skiprows=ROWS)\n",
    "#test = pd.read_csv(\"/export/storage_adgandhi/PBJhours_ML/Data/Intermediate/train_test_validation/testing_set.csv\",nrows=10)\n",
    "val = train[:int(ROWS*0.2)]\n",
    "test = train[int(ROWS*0.2):int(ROWS*0.3)]\n",
    "train = train[int(ROWS*0.3):]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prov_id', 'employee_id', 'job_title', 'pay_type', 'date', 'hours',\n",
       "       'week', 'day_of_week', 'hours_l1', 'employees_l1', 'hours_l2',\n",
       "       'employees_l2', 'hours_l3', 'employees_l3', 'hours_l4', 'employees_l4',\n",
       "       'hours_l5', 'employees_l5', 'hours_l6', 'employees_l6', 'hours_l7',\n",
       "       'employees_l7', 'hours_l8', 'employees_l8', 'hours_l9', 'employees_l9',\n",
       "       'hours_l10', 'employees_l10', 'hours_l11', 'employees_l11', 'hours_l12',\n",
       "       'employees_l12', 'hours_l13', 'employees_l13', 'hours_l14',\n",
       "       'employees_l14', 'hours_l15', 'employees_l15', 'hours_l16',\n",
       "       'employees_l16', 'hours_l17', 'employees_l17', 'hours_l18',\n",
       "       'employees_l18', 'hours_l19', 'employees_l19', 'hours_l20',\n",
       "       'employees_l20', 'hours_l21', 'employees_l21', 'hours_l22',\n",
       "       'employees_l22', 'hours_l23', 'employees_l23', 'hours_l24',\n",
       "       'employees_l24', 'hours_l25', 'employees_l25', 'hours_l26',\n",
       "       'employees_l26', 'hours_l27', 'employees_l27', 'hours_l28',\n",
       "       'employees_l28', 'hours_l29', 'employees_l29', 'hours_l30',\n",
       "       'employees_l30', 'week_perc0', 'week_perc1', 'week_perc2', 'week_perc3',\n",
       "       'week_perc4', 'week_perc5', 'week_perc6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          job_title  pay_type  day_of_week  hours_l1  employees_l1  hours_l2  \\\n",
      "32885957         35         2            6      0.00           5.0      0.00   \n",
      "32885958         35         2            5      0.00           5.0      6.10   \n",
      "32885959         35         2            4      6.10           5.0      6.00   \n",
      "32885960         35         2            3      6.00           5.0      6.10   \n",
      "32885961         35         2            2      6.10           5.0      0.00   \n",
      "...             ...       ...          ...       ...           ...       ...   \n",
      "99999995          3         3            3      7.65          32.0      7.55   \n",
      "99999996          3         3            2      7.55          32.0      0.50   \n",
      "99999997          3         3            1      0.50          34.0      0.50   \n",
      "99999998          3         3            1      7.86          27.0      7.81   \n",
      "99999999          3         3            0      7.81          27.0      6.55   \n",
      "\n",
      "          employees_l2  hours_l3  employees_l3  hours_l4  ...  employees_l29  \\\n",
      "32885957           5.0      6.10           5.0      6.00  ...            4.0   \n",
      "32885958           5.0      6.00           5.0      6.10  ...            4.0   \n",
      "32885959           5.0      6.10           5.0      0.00  ...            4.0   \n",
      "32885960           5.0      0.00           5.0      6.00  ...            4.0   \n",
      "32885961           5.0      6.00           5.0      6.10  ...            4.0   \n",
      "...                ...       ...           ...       ...  ...            ...   \n",
      "99999995          32.0      0.50          34.0      0.50  ...           29.0   \n",
      "99999996          34.0      0.50          33.0      7.79  ...           29.0   \n",
      "99999997          33.0      7.79          33.0      7.60  ...           30.0   \n",
      "99999998          27.0      6.55          27.0      0.00  ...           30.0   \n",
      "99999999          27.0      0.00          27.0      0.00  ...           31.0   \n",
      "\n",
      "          hours_l30  employees_l30  week_perc0  week_perc1  week_perc2  \\\n",
      "32885957       0.00            4.0        0.00        0.25        0.07   \n",
      "32885958       6.10            4.0        0.00        0.25        0.07   \n",
      "32885959       0.00            4.0        0.00        0.25        0.07   \n",
      "32885960       6.00            4.0        0.00        0.25        0.07   \n",
      "32885961       0.00            4.0        0.00        0.25        0.07   \n",
      "...             ...            ...         ...         ...         ...   \n",
      "99999995       7.60           29.0        0.15        0.13        0.18   \n",
      "99999996       7.82           30.0        0.15        0.12        0.17   \n",
      "99999997       7.87           29.0        0.15        0.12        0.17   \n",
      "99999998       7.59           31.0        0.16        0.14        0.14   \n",
      "99999999       7.45           30.0        0.16        0.14        0.14   \n",
      "\n",
      "          week_perc3  week_perc4  week_perc5  week_perc6  \n",
      "32885957        0.28        0.05        0.27        0.08  \n",
      "32885958        0.28        0.05        0.27        0.08  \n",
      "32885959        0.28        0.05        0.27        0.08  \n",
      "32885960        0.28        0.05        0.27        0.08  \n",
      "32885961        0.28        0.05        0.27        0.08  \n",
      "...              ...         ...         ...         ...  \n",
      "99999995        0.15        0.13        0.13        0.15  \n",
      "99999996        0.15        0.12        0.12        0.15  \n",
      "99999997        0.15        0.12        0.12        0.15  \n",
      "99999998        0.15        0.13        0.14        0.15  \n",
      "99999999        0.15        0.13        0.14        0.15  \n",
      "\n",
      "[60893388 rows x 70 columns]\n",
      "          hours\n",
      "32885957   0.00\n",
      "32885958   0.00\n",
      "32885959   0.00\n",
      "32885960   6.10\n",
      "32885961   6.00\n",
      "...         ...\n",
      "99999995   7.60\n",
      "99999996   7.65\n",
      "99999997   7.55\n",
      "99999998   0.00\n",
      "99999999   7.86\n",
      "\n",
      "[60893388 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ========================================\n",
    "#          PREP DATAFRAMES\n",
    "# ========================================\n",
    "#train = normalize(train)\n",
    "#val = normalize(val)\n",
    "train_inputs, train_labels = train.drop(['prov_id','employee_id','date','hours','week'], axis=1), train.filter(['hours'])\n",
    "val_inputs, val_labels = val.drop(['prov_id','employee_id','date','hours','week'], axis=1), val.filter(['hours'])\n",
    "test_inputs, test_labels = test.drop(['prov_id','employee_id','date','hours','week'], axis=1), test.filter(['hours'])\n",
    "\n",
    "print(train_inputs)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35  3 11 33 34 12 14 16]\n",
      "(60893388, 70)\n",
      "(20000000, 70)\n"
     ]
    }
   ],
   "source": [
    "train_inputs.dtypes\n",
    "print(train_inputs['job_title'].unique())\n",
    "print(train_inputs.shape)\n",
    "print(val_inputs.shape)\n",
    "#print(val_inputs['job_title'].unique())\n",
    "#print(test_inputs['job_title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_one_hot(labels,dataset):\n",
    "    outList = []\n",
    "    for label in labels:  \n",
    "        col = dataset[label]\n",
    "        ###Generate a dict for all unique values (Don't waste space encoding non important job id's)\n",
    "        map = {}\n",
    "        index = 0\n",
    "        for element in col.unique():\n",
    "            map[element] = index\n",
    "            index += 1\n",
    "        col = col.map(map)\n",
    "        tensor = tf.one_hot(col,len(col.unique()))\n",
    "        outList.append(tensor)\n",
    "        dataset = dataset.drop(columns=[label])\n",
    "    \n",
    "    outList.append(dataset)\n",
    "    output = tf.concat(outList,1)\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = expand_one_hot(['job_title','pay_type','day_of_week'],train_inputs)\n",
    "val_inputs = expand_one_hot(['job_title','pay_type','day_of_week'],val_inputs)\n",
    "#train_inputs = train_inputs.drop(['job_title','pay_type','day_of_week'],axis=1)\n",
    "#val_inputs = val_inputs.drop(['job_title','pay_type','day_of_week'],axis=1)\n",
    "test_inputs = expand_one_hot(['job_title','pay_type','day_of_week'],test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(60893388, 85), dtype=float32, numpy=\n",
       "array([[1.  , 0.  , 0.  , ..., 0.05, 0.27, 0.08],\n",
       "       [1.  , 0.  , 0.  , ..., 0.05, 0.27, 0.08],\n",
       "       [1.  , 0.  , 0.  , ..., 0.05, 0.27, 0.08],\n",
       "       ...,\n",
       "       [0.  , 1.  , 0.  , ..., 0.12, 0.12, 0.15],\n",
       "       [0.  , 1.  , 0.  , ..., 0.13, 0.14, 0.15],\n",
       "       [0.  , 1.  , 0.  , ..., 0.13, 0.14, 0.15]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 256\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 85), (None, 1)), types: (tf.float32, tf.float64)>\n"
     ]
    }
   ],
   "source": [
    "trainSet = tf.data.Dataset.from_tensor_slices((train_inputs,train_labels)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "valSet = tf.data.Dataset.from_tensor_slices((val_inputs,val_labels)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "testSet = tf.data.Dataset.from_tensor_slices((test_inputs,test_labels)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "print(trainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(256,activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(256,activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.6),\n",
    "        tf.keras.layers.Dense(128,activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(128,activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(64,activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(16),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[tf.keras.metrics.MeanAbsoluteError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_dir = './training_checkpointsBigModel'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                       save_weights_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "237865/237865 [==============================] - 21441s 90ms/step - loss: 10.5421 - mean_absolute_error: 2.2830 - val_loss: 11.7491 - val_mean_absolute_error: 2.4570\n",
      "Epoch 2/20\n",
      "237865/237865 [==============================] - 16716s 70ms/step - loss: 10.2613 - mean_absolute_error: 2.2288 - val_loss: 11.4821 - val_mean_absolute_error: 2.3944\n",
      "Epoch 3/20\n",
      "237865/237865 [==============================] - 16877s 71ms/step - loss: 10.1719 - mean_absolute_error: 2.2115 - val_loss: 11.4097 - val_mean_absolute_error: 2.4055\n",
      "Epoch 4/20\n",
      "237865/237865 [==============================] - 26855s 113ms/step - loss: 10.1059 - mean_absolute_error: 2.1994 - val_loss: 11.1169 - val_mean_absolute_error: 2.3884\n",
      "Epoch 5/20\n",
      " 43173/237865 [====>.........................] - ETA: 5:46:27 - loss: 10.5424 - mean_absolute_error: 2.3196"
     ]
    }
   ],
   "source": [
    "model.fit(trainSet, epochs=20, callbacks=callbacks, validation_data=valSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39063/39063 [==============================] - 366s 9ms/step - mean_absolute_error: 2.5386 - loss: 11.7877\n",
      "Eval loss: 11.787664413452148, Eval Accuracy: 2.538590669631958\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "eval_loss, eval_acc = model.evaluate(testSet)\n",
    "\n",
    "print('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
