{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "### Baseline model for RNN with embeddings\n",
    "\n",
    "## Inputs\n",
    "The input variables for the RNN in this notebook are the following:\n",
    "\n",
    "    1. Time series data -- hours_l1, hours_l2, hours_l3, ..., hours_l14\n",
    "    2. Additional predictors -- prov_id, day_of_week, avg_employees, perc_hours_today_before,\n",
    "       perc_hours_yesterday_before, perc_hours_tomorrow_before\n",
    "\n",
    "## Model Architecture\n",
    "The architecture used in this notebook combines an RNN with a feed forward neural network. The RNN layer recieves 14 days of lagged shift data and makes a prediction for the shift on the 15th day. This prediction is then concatenated with the additional predictors and fed into a traditional neural network to generate a better prediction--the idea being that the RNN (through it's long and short term memory) learns patterns over time and the FF network adjusts these patterns based on additional information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Train and Validation Sets. Time taken: 283.3759672641754\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "include_fields = ['hours','prov_id','day_of_week','avg_employees','perc_hours_today_before',\n",
    "                  'perc_hours_yesterday_before', 'perc_hours_tomorrow_before']\n",
    "for i in range(1,15):\n",
    "    include_fields.append(f\"hours_l{i}\")\n",
    "\n",
    "startTime = time.time()\n",
    "train = pd.read_csv(\"/export/storage_adgandhi/PBJhours_ML/Data/Intermediate/train_test_validation/training_set.csv\",usecols=include_fields).dropna()\n",
    "val = pd.read_csv(\"/export/storage_adgandhi/PBJhours_ML/Data/Intermediate/train_test_validation/crossvalidation_set.csv\",usecols=include_fields).dropna()\n",
    "print(f\"Loaded Train and Validation Sets. Time taken: {time.time()-startTime}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['prov_id', 'day_of_week', 'hours_l1', 'hours_l2', 'hours_l3',\n",
      "       'hours_l4', 'hours_l5', 'hours_l6', 'hours_l7', 'hours_l8', 'hours_l9',\n",
      "       'hours_l10', 'hours_l11', 'hours_l12', 'hours_l13', 'hours_l14',\n",
      "       'avg_employees', 'perc_hours_today_before',\n",
      "       'perc_hours_yesterday_before', 'perc_hours_tomorrow_before'],\n",
      "      dtype='object')\n",
      "Unique facilities: 15919\n"
     ]
    }
   ],
   "source": [
    "train_inputs, train_labels = train.drop(['hours'], axis=1), train.filter(['hours'])\n",
    "val_inputs, val_labels = val.drop(['hours'], axis=1), val.filter(['hours'])\n",
    "print(train_inputs.columns)\n",
    "vocab_size = len(train_inputs['prov_id'].unique())\n",
    "print(f\"Unique facilities: {vocab_size}\")\n",
    "\n",
    "#Remove providers that appear in val set but not train\n",
    "train_providers = train_inputs['prov_id'].unique()\n",
    "val_providers = val_inputs['prov_id'].unique()\n",
    "for value in val_providers:\n",
    "    if value not in train_providers:\n",
    "        mask = (val_inputs['prov_id']!=value)\n",
    "        val_inputs = val_inputs[mask]\n",
    "        val_labels = val_labels[mask]\n",
    "\n",
    "# Remap prov_id's between 0 - # providers\n",
    "provider_map = {}\n",
    "index = 0\n",
    "for element in train_inputs['prov_id'].unique():\n",
    "    provider_map[element]=index\n",
    "    index +=1\n",
    "train_inputs['prov_id'] = train_inputs['prov_id'].map(provider_map)\n",
    "val_inputs['prov_id'] = val_inputs['prov_id'].map(provider_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137722552, 26)\n",
      "(68726819, 26)\n",
      "tf.Tensor(\n",
      "[ 0.        0.        0.        0.        7.5       7.5       0.\n",
      "  0.        0.        0.        7.5       0.        7.5       0.\n",
      "  0.       56.857143  0.        0.        0.        1.        0.\n",
      "  0.        0.        0.        0.        0.      ], shape=(26,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def expand_one_hot(labels,dataset):\n",
    "    outList = []\n",
    "    for label in labels:  \n",
    "        col = dataset[label]\n",
    "        ###Generate a dict for all unique values (Don't waste space encoding non important job id's)\n",
    "        map = {}\n",
    "        index = 0\n",
    "        for element in col.unique():\n",
    "            map[element] = index\n",
    "            index += 1\n",
    "        col = col.map(map)\n",
    "        tensor = tf.one_hot(col,len(col.unique()))\n",
    "        outList.append(tensor)\n",
    "        dataset = dataset.drop(columns=[label])\n",
    "    \n",
    "    outList.insert(0,dataset)\n",
    "    output = tf.concat(outList,1)\n",
    "    return output\n",
    "\n",
    "train_inputs = expand_one_hot(['day_of_week'],train_inputs)\n",
    "val_inputs = expand_one_hot(['day_of_week'],val_inputs)\n",
    "#test_inputs = expand_one_hot(['day_of_week'],test_inputs)\n",
    "\n",
    "print(train_inputs.shape)\n",
    "print(val_inputs.shape)\n",
    "print(train_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 26), (None, 1)), types: (tf.float32, tf.float64)>\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 256\n",
    "trainSet = tf.data.Dataset.from_tensor_slices((train_inputs,train_labels)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "valSet = tf.data.Dataset.from_tensor_slices((val_inputs,val_labels)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "#testSet = tf.data.Dataset.from_tensor_slices((test_inputs,test_labels)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "print(trainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "\n",
    "    def __init__(self,vocab_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size,10)\n",
    "        self.lstm = tf.keras.layers.LSTM(64)\n",
    "        self.dense1 = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
    "        self.out = tf.keras.layers.Dense(1)\n",
    "        self.batch_norm = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        #prov_id is first column\n",
    "        embedding_vectors = self.embedding(inputs[:,0])\n",
    "        time_series = tf.expand_dims(inputs[:,1:15],2)\n",
    "        additional_inputs = inputs[:,15:]\n",
    "\n",
    "        x = self.lstm(time_series)\n",
    "        x = tf.concat([x,embedding_vectors,additional_inputs],1)\n",
    "        x = self.dense1(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.batch_norm(x)\n",
    "        return self.out(x)\n",
    "\n",
    "model = RNN(vocab_size)\n",
    "    \n",
    "model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=[tf.keras.metrics.MeanAbsoluteError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_dir = './training_checkpointsRNN'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "def decay(epoch):\n",
    "  if epoch < 3:\n",
    "    return 1e-4\n",
    "  elif epoch >= 3 and epoch < 7:\n",
    "    return 1e-5\n",
    "  else:\n",
    "    return 1e-6\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True),\n",
    "    tf.keras.callbacks.LearningRateScheduler(decay)\n",
    "]\n",
    "#model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "537979/537979 [==============================] - 9210s 17ms/step - loss: 10.9557 - mean_absolute_error: 2.3602 - val_loss: 12.5060 - val_mean_absolute_error: 2.8349\n",
      "Epoch 2/10\n",
      "537979/537979 [==============================] - 10032s 19ms/step - loss: 9.5490 - mean_absolute_error: 2.1071 - val_loss: 12.2351 - val_mean_absolute_error: 2.7508\n",
      "Epoch 3/10\n",
      "537979/537979 [==============================] - 11372s 21ms/step - loss: 9.3884 - mean_absolute_error: 2.0753 - val_loss: 11.8687 - val_mean_absolute_error: 2.6148\n",
      "Epoch 4/10\n",
      "537979/537979 [==============================] - 10418s 19ms/step - loss: 9.4134 - mean_absolute_error: 2.0777 - val_loss: 11.7949 - val_mean_absolute_error: 2.5777\n",
      "Epoch 5/10\n",
      "537979/537979 [==============================] - 11260s 21ms/step - loss: 9.3639 - mean_absolute_error: 2.0671 - val_loss: 11.7731 - val_mean_absolute_error: 2.5611\n",
      "Epoch 6/10\n",
      "537979/537979 [==============================] - 11316s 21ms/step - loss: 9.3436 - mean_absolute_error: 2.0633 - val_loss: 11.7613 - val_mean_absolute_error: 2.5502\n",
      "Epoch 7/10\n",
      "537979/537979 [==============================] - 12078s 22ms/step - loss: 9.3300 - mean_absolute_error: 2.0609 - val_loss: 11.7619 - val_mean_absolute_error: 2.5470\n",
      "Epoch 8/10\n",
      "537979/537979 [==============================] - 14084s 26ms/step - loss: 9.3556 - mean_absolute_error: 2.0708 - val_loss: 11.5503 - val_mean_absolute_error: 2.5093\n",
      "Epoch 9/10\n",
      "537979/537979 [==============================] - 18873s 35ms/step - loss: 9.3360 - mean_absolute_error: 2.0646 - val_loss: 11.5439 - val_mean_absolute_error: 2.5045\n",
      "Epoch 10/10\n",
      "537979/537979 [==============================] - 22957s 43ms/step - loss: 9.3327 - mean_absolute_error: 2.0637 - val_loss: 11.5369 - val_mean_absolute_error: 2.5053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fedc711d6d0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainSet, epochs=10, callbacks=callbacks, validation_data=valSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6",
   "language": "python",
   "name": "testproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
